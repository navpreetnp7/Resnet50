{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assgn4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m6suPnwIKOd",
        "outputId": "1abd682f-9a0a-43dd-a590-fd5c10e2662c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "!unzip -qq \"drive/My Drive/tiny-imagenet-200.zip\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2LoWYbbIVdN",
        "outputId": "2e45c022-ca7a-425d-a1d9-60e2469fe84b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import six\n",
        "from keras.models import Model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    GlobalAveragePooling2D,\n",
        "    Flatten\n",
        ")\n",
        "from keras.layers.convolutional import (\n",
        "    Conv2D,\n",
        "    Convolution2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D\n",
        "    \n",
        ")\n",
        "from keras.layers.merge import add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "from keras.callbacks import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQPpySHZJrhg",
        "outputId": "6ed21e08-df1a-42d8-f544-c53ca35e2b1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3794
        }
      },
      "source": [
        "def relu(x):\n",
        "    return Activation('relu')(x)\n",
        "\n",
        "def bn_relu_conv(f1,f2,stride):\n",
        "    def unit(x):\n",
        "        nBottleneckPlane = int(f2 / 4)\n",
        "        nbp = nBottleneckPlane\n",
        "        if f1==f2:\n",
        "            ident = x\n",
        "\n",
        "            x = BatchNormalization(axis=-1)(x)\n",
        "            x = relu(x)\n",
        "            x = Conv2D(nbp,(1,1),\n",
        "            strides=(stride,stride))(x)\n",
        "\n",
        "            x = BatchNormalization(axis=-1)(x)\n",
        "            x = relu(x)\n",
        "            x = Conv2D(nbp,(3,3),padding='same')(x)\n",
        "\n",
        "            x = BatchNormalization(axis=-1)(x)\n",
        "            x = relu(x)\n",
        "            x = Conv2D(f2,(1,1))(x)\n",
        "\n",
        "            out = add([ident, x])\n",
        "        else:\n",
        "            x = BatchNormalization(axis=-1)(x)\n",
        "            x = relu(x)\n",
        "            ident = x\n",
        "\n",
        "            x = Conv2D(nbp,(1,1),\n",
        "            strides=(stride,stride))(x)\n",
        "\n",
        "            x = BatchNormalization(axis=-1)(x)\n",
        "            x = relu(x)\n",
        "            x = Conv2D(nbp,(3,3),padding='same')(x)\n",
        "\n",
        "            x = BatchNormalization(axis=-1)(x)\n",
        "            x = relu(x)\n",
        "            x = Conv2D(f2,(1,1))(x)\n",
        "\n",
        "            ident = Conv2D(f2,(1,1),\n",
        "            strides=(stride,stride))(ident)\n",
        "\n",
        "            out = add([ident, x])\n",
        "\n",
        "        return out\n",
        "    return unit      \n",
        "\n",
        "def resblock(f1,f2,layers,std):\n",
        "    def unit(x):\n",
        "        for i in range(layers):\n",
        "            if i==0:\n",
        "                x = bn_relu_conv(f1,f2,std)(x)\n",
        "            else:\n",
        "                x = bn_relu_conv(f2,f2,1)(x)\n",
        "        return x\n",
        "    return unit      \n",
        "\n",
        "inp = Input(shape=(32,32,3))\n",
        "i = inp\n",
        "\n",
        "i = Conv2D(64,(3,3),padding='same')(i)\n",
        "\n",
        "i = resblock(64,128,3,1)(i) #32x32\n",
        "i = resblock(128,256,3,2)(i) #16x16\n",
        "i = resblock(256,512,3,2)(i) #8x8\n",
        "\n",
        "i = BatchNormalization(axis=-1)(i)\n",
        "i = relu(i)\n",
        "\n",
        "i = AveragePooling2D(pool_size=(8,8),padding='valid')(i) #1x1\n",
        "\n",
        "i = Conv2D(200,1)(i)\n",
        "#i = Flatten()(i) # 128\n",
        "i = GlobalAveragePooling2D()(i)\n",
        "\n",
        "#i = Dense(10)(i)\n",
        "i = Activation('softmax')(i)\n",
        "\n",
        "model = Model(inputs=inp,outputs=i)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 64)   1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 64)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 32)   2080        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 32)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 32)   9248        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 32)   128         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 32)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 128)  8320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 128)  4224        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 32, 32, 128)  0           conv2d_5[0][0]                   \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 128)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 32)   4128        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 32)   128         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 32)   9248        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 32)   128         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 32)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 128)  4224        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 32, 32, 128)  0           add_1[0][0]                      \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 128)  512         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 128)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 32)   4128        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 32)   128         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 32)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 32)   9248        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 32)   128         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 32)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 128)  4224        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 128)  0           add_2[0][0]                      \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 128)  512         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 128)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 64)   8256        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 64)   256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 16, 16, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 64)   36928       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 64)   256         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 16, 16, 64)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 256)  33024       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 256)  16640       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 16, 16, 256)  0           conv2d_15[0][0]                  \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 256)  1024        add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 256)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 64)   16448       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 64)   256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 64)   36928       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 64)   256         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 256)  16640       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 16, 16, 256)  0           add_4[0][0]                      \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 256)  1024        add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 256)  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 64)   16448       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 64)   256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 64)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 64)   36928       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 64)   256         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 64)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 256)  16640       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 16, 16, 256)  0           add_5[0][0]                      \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 256)  1024        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 256)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 128)    32896       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 128)    512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 8, 8, 128)    0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 128)    147584      activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 128)    512         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 8, 8, 128)    0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 8, 8, 512)    131584      activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 512)    66048       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 8, 8, 512)    0           conv2d_25[0][0]                  \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 8, 512)    2048        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 8, 8, 512)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 8, 128)    65664       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 8, 128)    512         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 8, 8, 128)    0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 128)    147584      activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 8, 8, 128)    512         conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 8, 8, 128)    0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 512)    66048       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 8, 8, 512)    0           add_7[0][0]                      \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 8, 512)    2048        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 8, 8, 512)    0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 128)    65664       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 128)    512         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 128)    0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 128)    147584      activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 128)    512         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 128)    0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 512)    66048       activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 8, 8, 512)    0           add_8[0][0]                      \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 512)    2048        add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 512)    0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 1, 1, 200)    102600      average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 200)          0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 200)          0           global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 1,351,432\n",
            "Trainable params: 1,343,240\n",
            "Non-trainable params: 8,192\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdZhGN0mJtN3",
        "outputId": "20864722-c725-4648-ef20-2f31dd585312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "#LOAD IMAGES\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "#Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
        "from keras.layers.normalization import BatchNormalization \n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.utils.visualize_util import plot\n",
        "import h5py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "def get_annotations_map():\n",
        "  \n",
        "  valAnnotationsPath = './32tiny-imagenet-200/tiny-imagenet-200/val/val_annotations.txt'\n",
        "  valAnnotationsFile = open(valAnnotationsPath, 'r')\n",
        "  valAnnotationsContents = valAnnotationsFile.read()\n",
        "  valAnnotations = {}\n",
        "  for line in valAnnotationsContents.splitlines():\n",
        "    pieces = line.strip().split()\n",
        "    valAnnotations[pieces[0]] = pieces[1]\n",
        "  return valAnnotations\n",
        "\n",
        "def load_images(path,num_classes):\n",
        "    #Load images\n",
        "    \n",
        "    print('Loading ' + str(num_classes) + ' classes')\n",
        "\n",
        "    X_train=np.zeros([num_classes*500,3,32,32],dtype='uint8')\n",
        "    y_train=np.zeros([num_classes*500], dtype='uint8')\n",
        "    trainPath=path+'/train'\n",
        "\n",
        "    print('loading training images...');\n",
        "\n",
        "    i=0\n",
        "    j=0\n",
        "    annotations={}\n",
        "    for sChild in os.listdir(trainPath):\n",
        "        sChildPath = os.path.join(os.path.join(trainPath,sChild),'images')\n",
        "        annotations[sChild]=j\n",
        "        for c in os.listdir(sChildPath):\n",
        "            X=np.array(Image.open(os.path.join(sChildPath,c)))\n",
        "            if len(np.shape(X))==2:\n",
        "                X_train[i]=np.array([X,X,X])\n",
        "            else:\n",
        "                X_train[i]=np.transpose(X,(2,0,1))\n",
        "            y_train[i]=j\n",
        "            i+=1\n",
        "        j+=1\n",
        "        if (j >= num_classes):\n",
        "            break\n",
        "\n",
        "    print('finished loading training images')\n",
        "\n",
        "    val_annotations_map = get_annotations_map()\n",
        "\n",
        "    X_test = np.zeros([num_classes*50,3,32,32],dtype='uint8')\n",
        "    y_test = np.zeros([num_classes*50], dtype='uint8')\n",
        "\n",
        "\n",
        "    print('loading test images...')\n",
        "\n",
        "    i = 0\n",
        "    testPath=path+'/val/images'\n",
        "    for sChild in os.listdir(testPath):\n",
        "        if val_annotations_map[sChild] in annotations.keys():\n",
        "            sChildPath = os.path.join(testPath, sChild)\n",
        "            X=np.array(Image.open(sChildPath))\n",
        "            if len(np.shape(X))==2:\n",
        "                X_test[i]=np.array([X,X,X])\n",
        "            else:\n",
        "                X_test[i]=np.transpose(X,(2,0,1))\n",
        "            y_test[i]=annotations[val_annotations_map[sChild]]\n",
        "            i+=1\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "\n",
        "    print('finished loading test images ',end=\"\")\n",
        "    print(i)\n",
        "\n",
        "    return X_train,y_train,X_test,y_test\n",
        "\n",
        "loss_functions = ['categorical_crossentropy','squared_hinge','hinge']\n",
        "num_classes = 200\n",
        "batch_size = 128\n",
        "nb_epoch = 30\n",
        "\n",
        "#Load images\n",
        "path='./32tiny-imagenet-200/tiny-imagenet-200/'\n",
        "X_train,y_train,X_test,y_test=load_images(path,num_classes)\n",
        "\n",
        "X_train = X_train.transpose(0,3,1,2)\n",
        "X_test = X_test.transpose(0,3,1,2)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print('X_test shape:', X_test.shape)\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "num_samples=len(X_train)\n",
        "\n",
        "# input image dimensions\n",
        "num_channels , img_rows, img_cols = X_train.shape[1], X_train.shape[2], X_train.shape[3]\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading 200 classes\n",
            "loading training images...\n",
            "finished loading training images\n",
            "loading test images...\n",
            "finished loading test images 10000\n",
            "X_train shape: (100000, 32, 3, 32)\n",
            "100000 train samples\n",
            "X_test shape: (10000, 32, 3, 32)\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY6xX8HJJuLH",
        "outputId": "b7bf2700-4e2c-421a-bba3-dc50c3981ac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_train = X_train.transpose(0,3,1,2)\n",
        "X_test = X_test.transpose(0,3,1,2)\n",
        "print (X_train.shape)\n",
        "print (X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xTHpCEiJvoQ",
        "outputId": "368a2a67-8910-4b54-d36d-c0f8317202fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"./drive/My Drive/ChkPoint3/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# W/O ImgAug\n",
        "\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          validation_data=(X_test, Y_test),\n",
        "          shuffle=True,\n",
        "          callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 100000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 259s 3ms/step - loss: 4.3157 - acc: 0.0986 - val_loss: 4.4449 - val_acc: 0.0984\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.09840, saving model to ./drive/My Drive/ChkPoint3/epochs:001-val_acc:0.098.hdf5\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 251s 3ms/step - loss: 3.5479 - acc: 0.2027 - val_loss: 3.7905 - val_acc: 0.1712\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.09840 to 0.17120, saving model to ./drive/My Drive/ChkPoint3/epochs:002-val_acc:0.171.hdf5\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 250s 3ms/step - loss: 3.1862 - acc: 0.2650 - val_loss: 3.5069 - val_acc: 0.2161\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.17120 to 0.21610, saving model to ./drive/My Drive/ChkPoint3/epochs:003-val_acc:0.216.hdf5\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 250s 3ms/step - loss: 2.9317 - acc: 0.3118 - val_loss: 3.2766 - val_acc: 0.2591\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.21610 to 0.25910, saving model to ./drive/My Drive/ChkPoint3/epochs:004-val_acc:0.259.hdf5\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 250s 3ms/step - loss: 2.7274 - acc: 0.3498 - val_loss: 3.1006 - val_acc: 0.2921\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.25910 to 0.29210, saving model to ./drive/My Drive/ChkPoint3/epochs:005-val_acc:0.292.hdf5\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 250s 2ms/step - loss: 2.5489 - acc: 0.3847 - val_loss: 3.0638 - val_acc: 0.3008\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.29210 to 0.30080, saving model to ./drive/My Drive/ChkPoint3/epochs:006-val_acc:0.301.hdf5\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 250s 3ms/step - loss: 2.3835 - acc: 0.4168 - val_loss: 2.9242 - val_acc: 0.3259\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.30080 to 0.32590, saving model to ./drive/My Drive/ChkPoint3/epochs:007-val_acc:0.326.hdf5\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 251s 3ms/step - loss: 2.2401 - acc: 0.4475 - val_loss: 3.0168 - val_acc: 0.3161\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.32590\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 251s 3ms/step - loss: 2.0924 - acc: 0.4738 - val_loss: 3.1545 - val_acc: 0.3082\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.32590\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 251s 3ms/step - loss: 1.9498 - acc: 0.5063 - val_loss: 3.1292 - val_acc: 0.3154\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.32590\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7ddd8c9c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHc81E28L6K8",
        "outputId": "646fa1d8-d400-4f83-e0ba-3c1b2a9446f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "def relu(x):\n",
        "    return Activation('relu')(x)\n",
        "\n",
        "def bn_relu_conv(f1,f2,stride):\n",
        "    def unit(x):\n",
        "        nBottleneckPlane = int(f2 / 4)\n",
        "        nbp = nBottleneckPlane\n",
        "        if f1==f2:\n",
        "            ident = x\n",
        "\n",
        "            x = BatchNormalization(axis=-1)(x)\n",
        "            x = relu(x)\n",
        "            x = Conv2D(nbp,(1,1),\n",
        "            strides=(stride,stride))(x)\n",
        "\n",
        "            x = BatchNormalization(axis=-1)(x)\n",
        "            x = relu(x)\n",
        "            x = Conv2D(nbp,(3,3),padding='same')(x)\n",
        "\n",
        "            x = BatchNormalization(axis=-1)(x)\n",
        "            x = relu(x)\n",
        "            x = Conv2D(f2,(1,1))(x)\n",
        "\n",
        "            out = add([ident, x])\n",
        "        else:\n",
        "            x = BatchNormalization(axis=-1)(x)\n",
        "            x = relu(x)\n",
        "            ident = x\n",
        "\n",
        "            x = Conv2D(nbp,(1,1),\n",
        "            strides=(stride,stride))(x)\n",
        "\n",
        "            x = BatchNormalization(axis=-1)(x)\n",
        "            x = relu(x)\n",
        "            x = Conv2D(nbp,(3,3),padding='same')(x)\n",
        "\n",
        "            x = BatchNormalization(axis=-1)(x)\n",
        "            x = relu(x)\n",
        "            x = Conv2D(f2,(1,1))(x)\n",
        "\n",
        "            ident = Conv2D(f2,(1,1),\n",
        "            strides=(stride,stride))(ident)\n",
        "\n",
        "            out = add([ident, x])\n",
        "\n",
        "        return out\n",
        "    return unit      \n",
        "\n",
        "def resblock(f1,f2,layers,std):\n",
        "    def unit(x):\n",
        "        for i in range(layers):\n",
        "            if i==0:\n",
        "                x = bn_relu_conv(f1,f2,std)(x)\n",
        "            else:\n",
        "                x = bn_relu_conv(f2,f2,1)(x)\n",
        "        return x\n",
        "    return unit      \n",
        "\n",
        "inp = Input(shape=(64,64,3))\n",
        "i = inp\n",
        "\n",
        "i = Conv2D(64,(3,3),padding='same')(i)\n",
        "\n",
        "i = resblock(64,128,3,1)(i) #32x32\n",
        "i = resblock(128,256,3,2)(i) #16x16\n",
        "i = resblock(256,512,3,2)(i) #8x8\n",
        "\n",
        "i = BatchNormalization(axis=-1)(i)\n",
        "i = relu(i)\n",
        "\n",
        "i = AveragePooling2D(pool_size=(8,8),padding='valid')(i) #1x1\n",
        "\n",
        "i = Conv2D(200,1)(i)\n",
        "#i = Flatten()(i) # 128\n",
        "i = GlobalAveragePooling2D()(i)\n",
        "\n",
        "#i = Dense(10)(i)\n",
        "i = Activation('softmax')(i)\n",
        "\n",
        "model = Model(inputs=inp,outputs=i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM6wXo_VJwjD"
      },
      "source": [
        "model.load_weights('./drive/My Drive/ChkPoint3/epochs:007-val_acc:0.326.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZnCsgVhLw0A",
        "outputId": "f77c1956-9abb-4958-d35f-def72dee0713",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "#LOAD IMAGES\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "\n",
        "#Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Flatten\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D, AveragePooling2D\n",
        "from keras.layers.normalization import BatchNormalization \n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.utils.visualize_util import plot\n",
        "import h5py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def get_annotations_map():\n",
        "  \n",
        "  valAnnotationsPath = './tiny-imagenet-200/val/val_annotations.txt'\n",
        "  valAnnotationsFile = open(valAnnotationsPath, 'r')\n",
        "  valAnnotationsContents = valAnnotationsFile.read()\n",
        "  valAnnotations = {}\n",
        "  for line in valAnnotationsContents.splitlines():\n",
        "    pieces = line.strip().split()\n",
        "    valAnnotations[pieces[0]] = pieces[1]\n",
        "  return valAnnotations\n",
        "\n",
        "def load_images(path,num_classes):\n",
        "    #Load images\n",
        "    \n",
        "    print('Loading ' + str(num_classes) + ' classes')\n",
        "\n",
        "    X_train=np.zeros([num_classes*500,3,64,64],dtype='uint8')\n",
        "    y_train=np.zeros([num_classes*500], dtype='uint8')\n",
        "    trainPath=path+'/train'\n",
        "\n",
        "    print('loading training images...');\n",
        "\n",
        "    i=0\n",
        "    j=0\n",
        "    annotations={}\n",
        "    for sChild in os.listdir(trainPath):\n",
        "        sChildPath = os.path.join(os.path.join(trainPath,sChild),'images')\n",
        "        annotations[sChild]=j\n",
        "        for c in os.listdir(sChildPath):\n",
        "            X=np.array(Image.open(os.path.join(sChildPath,c)))\n",
        "            if len(np.shape(X))==2:\n",
        "                X_train[i]=np.array([X,X,X])\n",
        "            else:\n",
        "                X_train[i]=np.transpose(X,(2,0,1))\n",
        "            y_train[i]=j\n",
        "            i+=1\n",
        "        j+=1\n",
        "        if (j >= num_classes):\n",
        "            break\n",
        "\n",
        "    print('finished loading training images')\n",
        "\n",
        "    val_annotations_map = get_annotations_map()\n",
        "\n",
        "    X_test = np.zeros([num_classes*50,3,64,64],dtype='uint8')\n",
        "    y_test = np.zeros([num_classes*50], dtype='uint8')\n",
        "\n",
        "\n",
        "    print('loading test images...')\n",
        "\n",
        "    i = 0\n",
        "    testPath=path+'/val/images'\n",
        "    for sChild in os.listdir(testPath):\n",
        "        if val_annotations_map[sChild] in annotations.keys():\n",
        "            sChildPath = os.path.join(testPath, sChild)\n",
        "            X=np.array(Image.open(sChildPath))\n",
        "            if len(np.shape(X))==2:\n",
        "                X_test[i]=np.array([X,X,X])\n",
        "            else:\n",
        "                X_test[i]=np.transpose(X,(2,0,1))\n",
        "            y_test[i]=annotations[val_annotations_map[sChild]]\n",
        "            i+=1\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "\n",
        "    print('finished loading test images ',end=\"\")\n",
        "    print(i)\n",
        "\n",
        "    return X_train,y_train,X_test,y_test\n",
        "\n",
        "loss_functions = ['categorical_crossentropy','squared_hinge','hinge']\n",
        "num_classes = 200\n",
        "batch_size = 128\n",
        "nb_epoch = 30\n",
        "\n",
        "#Load images\n",
        "path='./tiny-imagenet-200/'\n",
        "X_train,y_train,X_test,y_test=load_images(path,num_classes)\n",
        "\n",
        "X_train = X_train.transpose(0,3,1,2)\n",
        "X_test = X_test.transpose(0,3,1,2)\n",
        "\n",
        "print('X_train shape:', X_train.shape)\n",
        "print(X_train.shape[0], 'train samples')\n",
        "print('X_test shape:', X_test.shape)\n",
        "print(X_test.shape[0], 'test samples')\n",
        "\n",
        "num_samples=len(X_train)\n",
        "\n",
        "# input image dimensions\n",
        "num_channels , img_rows, img_cols = X_train.shape[1], X_train.shape[2], X_train.shape[3]\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "X_train = X_train.transpose(0,3,1,2)\n",
        "X_test = X_test.transpose(0,3,1,2)\n",
        "print (X_train.shape)\n",
        "print (X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading 200 classes\n",
            "loading training images...\n",
            "finished loading training images\n",
            "loading test images...\n",
            "finished loading test images 10000\n",
            "X_train shape: (100000, 64, 3, 64)\n",
            "100000 train samples\n",
            "X_test shape: (10000, 64, 3, 64)\n",
            "10000 test samples\n",
            "(100000, 64, 64, 3)\n",
            "(10000, 64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLPeVaAELz9G",
        "outputId": "6b4279f1-ae66-4f73-cf44-2581c31e14b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"./drive/My Drive/ChkPoint3/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# W/O ImgAug\n",
        "\n",
        "model.fit(X_train, Y_train,\n",
        "          batch_size=128,\n",
        "          epochs=10,\n",
        "          validation_data=(X_test, Y_test),\n",
        "          shuffle=True,\n",
        "          callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 100000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 769s 8ms/step - loss: 2.4989 - acc: 0.3982 - val_loss: 2.7703 - val_acc: 0.3553\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.35530, saving model to ./drive/My Drive/ChkPoint3/epochs:001-val_acc:0.355.hdf5\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 761s 8ms/step - loss: 2.2490 - acc: 0.4492 - val_loss: 2.8449 - val_acc: 0.3418\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.35530\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 760s 8ms/step - loss: 2.0905 - acc: 0.4828 - val_loss: 2.6559 - val_acc: 0.3719\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.35530 to 0.37190, saving model to ./drive/My Drive/ChkPoint3/epochs:003-val_acc:0.372.hdf5\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 760s 8ms/step - loss: 1.9594 - acc: 0.5097 - val_loss: 2.8023 - val_acc: 0.3803\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.37190 to 0.38030, saving model to ./drive/My Drive/ChkPoint3/epochs:004-val_acc:0.380.hdf5\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 760s 8ms/step - loss: 1.8320 - acc: 0.5346 - val_loss: 2.7534 - val_acc: 0.3776\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.38030\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 759s 8ms/step - loss: 1.7145 - acc: 0.5605 - val_loss: 2.6322 - val_acc: 0.3957\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.38030 to 0.39570, saving model to ./drive/My Drive/ChkPoint3/epochs:006-val_acc:0.396.hdf5\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 759s 8ms/step - loss: 1.6006 - acc: 0.5848 - val_loss: 2.4733 - val_acc: 0.4241\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.39570 to 0.42410, saving model to ./drive/My Drive/ChkPoint3/epochs:007-val_acc:0.424.hdf5\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 759s 8ms/step - loss: 1.4931 - acc: 0.6088 - val_loss: 2.6628 - val_acc: 0.3946\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.42410\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 761s 8ms/step - loss: 1.3876 - acc: 0.6318 - val_loss: 2.9507 - val_acc: 0.3791\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.42410\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 758s 8ms/step - loss: 1.2833 - acc: 0.6551 - val_loss: 2.5481 - val_acc: 0.4199\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.42410\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f66ab833080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCFMI4zi29Fg",
        "outputId": "1f4a1a7a-7cc5-4d79-b87d-5a1b9a707039",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2478
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"./drive/My Drive/ChkPoint3/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# ImgAug and start training\n",
        "\n",
        "print('Using real-time data augmentation.')\n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "\tfeaturewise_center=False,  # set input mean to 0 over the dataset\n",
        "\tsamplewise_center=False,  # set each sample mean to 0\n",
        "\tfeaturewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "\tsamplewise_std_normalization=False,  # divide each input by its std\n",
        "\tzca_whitening=False,  # apply ZCA whitening\n",
        "\trotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "\twidth_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "\theight_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "\thorizontal_flip=True,  # randomly flip images\n",
        "\tvertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=128),\n",
        "\t\t\t\t\tsteps_per_epoch=X_train.shape[0] // batch_size,\n",
        "\t\t\t\t\tvalidation_data=(X_test, Y_test),\n",
        "\t\t\t\t\tepochs=50, verbose=1, max_q_size=100,\n",
        "\t\t\t\t\tcallbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=781, validation_data=(array([[[..., epochs=50, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "781/781 [==============================] - 770s 986ms/step - loss: 1.9749 - acc: 0.5070 - val_loss: 2.6723 - val_acc: 0.4070\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.40700, saving model to ./drive/My Drive/ChkPoint3/epochs:001-val_acc:0.407.hdf5\n",
            "Epoch 2/50\n",
            "781/781 [==============================] - 755s 967ms/step - loss: 1.8542 - acc: 0.5329 - val_loss: 2.8092 - val_acc: 0.3878\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.40700\n",
            "Epoch 3/50\n",
            "781/781 [==============================] - 756s 968ms/step - loss: 1.7855 - acc: 0.5461 - val_loss: 2.3871 - val_acc: 0.4413\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.40700 to 0.44130, saving model to ./drive/My Drive/ChkPoint3/epochs:003-val_acc:0.441.hdf5\n",
            "Epoch 4/50\n",
            "781/781 [==============================] - 756s 968ms/step - loss: 1.7386 - acc: 0.5570 - val_loss: 2.5297 - val_acc: 0.4209\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.44130\n",
            "Epoch 5/50\n",
            "781/781 [==============================] - 755s 967ms/step - loss: 1.6944 - acc: 0.5668 - val_loss: 2.4296 - val_acc: 0.4434\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.44130 to 0.44340, saving model to ./drive/My Drive/ChkPoint3/epochs:005-val_acc:0.443.hdf5\n",
            "Epoch 6/50\n",
            "781/781 [==============================] - 755s 966ms/step - loss: 1.6505 - acc: 0.5763 - val_loss: 2.5073 - val_acc: 0.4381\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.44340\n",
            "Epoch 7/50\n",
            "781/781 [==============================] - 755s 967ms/step - loss: 1.6080 - acc: 0.5864 - val_loss: 2.4865 - val_acc: 0.4469\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.44340 to 0.44690, saving model to ./drive/My Drive/ChkPoint3/epochs:007-val_acc:0.447.hdf5\n",
            "Epoch 8/50\n",
            "781/781 [==============================] - 755s 967ms/step - loss: 1.5697 - acc: 0.5954 - val_loss: 2.3356 - val_acc: 0.4612\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.44690 to 0.46120, saving model to ./drive/My Drive/ChkPoint3/epochs:008-val_acc:0.461.hdf5\n",
            "Epoch 9/50\n",
            "781/781 [==============================] - 754s 966ms/step - loss: 1.5255 - acc: 0.6049 - val_loss: 2.3079 - val_acc: 0.4694\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.46120 to 0.46940, saving model to ./drive/My Drive/ChkPoint3/epochs:009-val_acc:0.469.hdf5\n",
            "Epoch 10/50\n",
            "781/781 [==============================] - 755s 967ms/step - loss: 1.4982 - acc: 0.6106 - val_loss: 2.4055 - val_acc: 0.4580\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.46940\n",
            "Epoch 11/50\n",
            "781/781 [==============================] - 755s 967ms/step - loss: 1.4530 - acc: 0.6218 - val_loss: 2.4996 - val_acc: 0.4450\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.46940\n",
            "Epoch 12/50\n",
            "781/781 [==============================] - 755s 967ms/step - loss: 1.4164 - acc: 0.6289 - val_loss: 2.3342 - val_acc: 0.4826\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.46940 to 0.48260, saving model to ./drive/My Drive/ChkPoint3/epochs:012-val_acc:0.483.hdf5\n",
            "Epoch 13/50\n",
            "781/781 [==============================] - 756s 967ms/step - loss: 1.3784 - acc: 0.6380 - val_loss: 2.3987 - val_acc: 0.4671\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.48260\n",
            "Epoch 14/50\n",
            "781/781 [==============================] - 756s 968ms/step - loss: 1.3477 - acc: 0.6445 - val_loss: 2.4536 - val_acc: 0.4583\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.48260\n",
            "Epoch 15/50\n",
            "781/781 [==============================] - 757s 970ms/step - loss: 1.3189 - acc: 0.6501 - val_loss: 2.4163 - val_acc: 0.4715\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.48260\n",
            "Epoch 16/50\n",
            "781/781 [==============================] - 758s 970ms/step - loss: 1.2831 - acc: 0.6577 - val_loss: 2.4176 - val_acc: 0.4786\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.48260\n",
            "Epoch 17/50\n",
            "781/781 [==============================] - 757s 969ms/step - loss: 1.2537 - acc: 0.6661 - val_loss: 2.2705 - val_acc: 0.4903\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.48260 to 0.49030, saving model to ./drive/My Drive/ChkPoint3/epochs:017-val_acc:0.490.hdf5\n",
            "Epoch 18/50\n",
            "781/781 [==============================] - 757s 969ms/step - loss: 1.2225 - acc: 0.6714 - val_loss: 2.3107 - val_acc: 0.4789\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.49030\n",
            "Epoch 19/50\n",
            "781/781 [==============================] - 757s 969ms/step - loss: 1.1922 - acc: 0.6786 - val_loss: 2.3903 - val_acc: 0.4843\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.49030\n",
            "Epoch 20/50\n",
            "781/781 [==============================] - 757s 969ms/step - loss: 1.1652 - acc: 0.6848 - val_loss: 2.4589 - val_acc: 0.4780\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.49030\n",
            "Epoch 21/50\n",
            "160/781 [=====>........................] - ETA: 9:37 - loss: 1.0908 - acc: 0.7013"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-6336cb8d857b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \t\t\t\t\tcallbacks=callbacks_list)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGTbKW5_Sbhu"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model.save('./drive/My Drive/my_model1.h5')  # creates a HDF5 file 'my_model.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbv2IvwTNG1c",
        "outputId": "c034e88b-4fec-4503-cb43-b333b468f271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=128),\n",
        "\t\t\t\t\tsteps_per_epoch=X_train.shape[0] // batch_size,\n",
        "\t\t\t\t\tvalidation_data=(X_test, Y_test),\n",
        "\t\t\t\t\tepochs=10, verbose=1, max_q_size=100,\n",
        "\t\t\t\t\tcallbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=781, validation_data=(array([[[..., epochs=10, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "781/781 [==============================] - 756s 968ms/step - loss: 1.1328 - acc: 0.6918 - val_loss: 2.5347 - val_acc: 0.4718\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.49030\n",
            "Epoch 2/10\n",
            "  4/781 [..............................] - ETA: 10:01 - loss: 1.0406 - acc: 0.7168"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_7USS00Ml1c",
        "outputId": "6a11f0b5-2b27-44a7-81dc-e55b1e765a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "# returns a compiled model\n",
        "# identical to the previous one\n",
        "model = load_model('./drive/My Drive/my_model1.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVJE7Dr3wM_7",
        "outputId": "7c841c11-58ce-4e57-cecb-cf840c07ddc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2206
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath=\"./drive/My Drive/ChkPoint3/epochs:{epoch:03d}-val_acc:{val_acc:.3f}.hdf5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "\n",
        "# ImgAug and start training\n",
        "\n",
        "print('Using real-time data augmentation.')\n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "\tfeaturewise_center=False,  # set input mean to 0 over the dataset\n",
        "\tsamplewise_center=False,  # set each sample mean to 0\n",
        "\tfeaturewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "\tsamplewise_std_normalization=False,  # divide each input by its std\n",
        "\tzca_whitening=False,  # apply ZCA whitening\n",
        "\trotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "\twidth_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "\theight_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "\thorizontal_flip=True,  # randomly flip images\n",
        "\tvertical_flip=False)  # randomly flip images\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model.fit_generator(datagen.flow(X_train, Y_train, batch_size=128),\n",
        "\t\t\t\t\tsteps_per_epoch=X_train.shape[0] // batch_size,\n",
        "\t\t\t\t\tvalidation_data=(X_test, Y_test),\n",
        "\t\t\t\t\tepochs=50, verbose=1, max_q_size=100,\n",
        "\t\t\t\t\tcallbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using real-time data augmentation.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., steps_per_epoch=781, validation_data=(array([[[..., epochs=50, verbose=1, callbacks=[<keras.ca..., max_queue_size=100)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "781/781 [==============================] - 828s 1s/step - loss: 1.1306 - acc: 0.6930 - val_loss: 2.6151 - val_acc: 0.4628\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.46280, saving model to ./drive/My Drive/ChkPoint3/epochs:001-val_acc:0.463.hdf5\n",
            "Epoch 2/50\n",
            "781/781 [==============================] - 816s 1s/step - loss: 1.1027 - acc: 0.7001 - val_loss: 2.6139 - val_acc: 0.4737\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.46280 to 0.47370, saving model to ./drive/My Drive/ChkPoint3/epochs:002-val_acc:0.474.hdf5\n",
            "Epoch 3/50\n",
            "781/781 [==============================] - 815s 1s/step - loss: 1.0733 - acc: 0.7063 - val_loss: 2.5028 - val_acc: 0.4832\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.47370 to 0.48320, saving model to ./drive/My Drive/ChkPoint3/epochs:003-val_acc:0.483.hdf5\n",
            "Epoch 4/50\n",
            "781/781 [==============================] - 815s 1s/step - loss: 1.0553 - acc: 0.7109 - val_loss: 2.4660 - val_acc: 0.4881\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.48320 to 0.48810, saving model to ./drive/My Drive/ChkPoint3/epochs:004-val_acc:0.488.hdf5\n",
            "Epoch 5/50\n",
            "781/781 [==============================] - 813s 1s/step - loss: 1.0223 - acc: 0.7182 - val_loss: 2.5318 - val_acc: 0.4827\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.48810\n",
            "Epoch 6/50\n",
            "781/781 [==============================] - 813s 1s/step - loss: 0.9957 - acc: 0.7247 - val_loss: 2.5033 - val_acc: 0.4875\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.48810\n",
            "Epoch 7/50\n",
            "781/781 [==============================] - 815s 1s/step - loss: 0.9751 - acc: 0.7292 - val_loss: 2.6550 - val_acc: 0.4645\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.48810\n",
            "Epoch 8/50\n",
            "781/781 [==============================] - 815s 1s/step - loss: 0.9484 - acc: 0.7354 - val_loss: 2.4200 - val_acc: 0.4981\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.48810 to 0.49810, saving model to ./drive/My Drive/ChkPoint3/epochs:008-val_acc:0.498.hdf5\n",
            "Epoch 9/50\n",
            "781/781 [==============================] - 815s 1s/step - loss: 0.9273 - acc: 0.7412 - val_loss: 2.8393 - val_acc: 0.4567\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.49810\n",
            "Epoch 10/50\n",
            "781/781 [==============================] - 816s 1s/step - loss: 0.9063 - acc: 0.7466 - val_loss: 2.6740 - val_acc: 0.4812\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.49810\n",
            "Epoch 11/50\n",
            "781/781 [==============================] - 815s 1s/step - loss: 0.8843 - acc: 0.7504 - val_loss: 2.7893 - val_acc: 0.4683\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.49810\n",
            "Epoch 12/50\n",
            "781/781 [==============================] - 816s 1s/step - loss: 0.8602 - acc: 0.7581 - val_loss: 2.6234 - val_acc: 0.4822\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.49810\n",
            "Epoch 13/50\n",
            "781/781 [==============================] - 816s 1s/step - loss: 0.8398 - acc: 0.7610 - val_loss: 2.5989 - val_acc: 0.4889\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.49810\n",
            "Epoch 14/50\n",
            "781/781 [==============================] - 820s 1s/step - loss: 0.8162 - acc: 0.7668 - val_loss: 2.6567 - val_acc: 0.4818\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.49810\n",
            "Epoch 15/50\n",
            "781/781 [==============================] - 821s 1s/step - loss: 0.8019 - acc: 0.7707 - val_loss: 2.6172 - val_acc: 0.4903\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.49810\n",
            "Epoch 16/50\n",
            "781/781 [==============================] - 820s 1s/step - loss: 0.7766 - acc: 0.7766 - val_loss: 2.7137 - val_acc: 0.4750\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.49810\n",
            "Epoch 17/50\n",
            "399/781 [==============>...............] - ETA: 6:26 - loss: 0.7290 - acc: 0.7907"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-451aa4328161>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \t\t\t\t\tcallbacks=callbacks_list)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkVpbU93wYrH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}